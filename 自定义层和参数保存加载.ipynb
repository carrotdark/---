{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "468c2531",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-2.5748e-01, -1.9514e-02,  2.2997e-01, -5.2434e-02, -1.2737e-01,\n",
      "          8.0027e-01,  3.9899e-01,  3.8181e-01, -5.4731e-01,  1.9903e-01,\n",
      "          2.5995e-01, -1.6573e-01, -4.8794e-01, -2.3635e-01,  3.4394e-01,\n",
      "          6.1264e-02,  2.6811e-02, -1.0170e-01,  1.6806e-01, -3.5956e-01,\n",
      "         -3.2735e-01, -3.6488e-01, -2.7775e-02, -6.5512e-01,  4.5217e-02,\n",
      "         -7.4495e-02, -2.6951e-01, -2.6565e-01, -3.2221e-01, -4.6450e-02,\n",
      "          1.2108e-01,  1.8254e-01, -6.3297e-01,  9.3186e-02,  3.3132e-01,\n",
      "          2.6284e-01,  2.6009e-01, -7.1419e-01,  2.1833e-01,  5.7977e-02,\n",
      "          2.4287e-01, -1.2702e-01,  4.0568e-01,  9.7700e-02, -1.5514e-01,\n",
      "          5.3977e-01, -3.8786e-01, -2.0130e-01, -2.6092e-01, -1.3869e-02,\n",
      "          3.2287e-01,  2.2958e-02, -1.4447e-01,  6.0348e-01,  1.0386e-01,\n",
      "         -1.7108e-01, -6.2856e-01,  4.9079e-01,  8.4828e-02,  3.7007e-01,\n",
      "          3.1681e-01,  1.3684e-01,  6.2040e-03,  4.8857e-01, -2.0883e-01,\n",
      "          3.7843e-02, -3.6009e-01, -4.3498e-02,  3.1819e-01,  1.2939e-01,\n",
      "         -9.2181e-02, -1.4103e-01, -1.7942e-01,  2.2526e-02,  2.9264e-01,\n",
      "          2.3469e-01,  9.2912e-02,  3.5888e-01,  4.2076e-01, -3.7195e-01,\n",
      "          2.2758e-01,  5.1491e-02, -5.0206e-01, -1.3161e-01, -4.7891e-01,\n",
      "          1.6594e-01,  3.9196e-01,  5.3022e-01,  5.5275e-01, -7.0580e-01,\n",
      "          7.5142e-01, -7.2227e-01,  6.7537e-01, -4.8251e-02, -3.5481e-01,\n",
      "          2.0344e-01, -1.2958e-01,  3.6950e-02,  9.2436e-03, -5.2251e-01,\n",
      "          5.6266e-01,  1.3001e-01, -6.1802e-01,  4.2360e-01,  3.6347e-01,\n",
      "          1.9819e-01, -1.1400e-01, -1.1477e-01,  2.3294e-01, -1.5407e-01,\n",
      "         -5.5540e-01,  2.0421e-01,  2.0710e-01, -5.9233e-01,  4.7691e-01,\n",
      "          7.8664e-02, -1.3767e-01, -2.0915e-02, -2.8369e-01, -2.2320e-03,\n",
      "         -1.5248e-01, -1.1321e-01,  2.5350e-01, -1.9914e-01, -7.0093e-01,\n",
      "          1.5005e-01,  2.1357e-01,  1.2097e-01],\n",
      "        [-7.3639e-01,  1.9719e-01,  5.3512e-01,  9.5782e-02, -3.3747e-01,\n",
      "          6.5480e-01,  2.9651e-01,  5.8058e-01, -1.7358e-01,  2.3520e-01,\n",
      "          2.4324e-01, -3.0867e-01, -4.8401e-01, -1.0324e-01,  1.2117e-01,\n",
      "          1.4289e-01, -2.8215e-01, -1.9995e-01,  4.9377e-01, -1.1416e-01,\n",
      "         -1.9559e-01, -5.3657e-02,  2.1138e-01, -4.2131e-01, -5.1018e-01,\n",
      "          2.0741e-01, -6.0534e-02, -6.2323e-01, -7.6328e-02,  1.4815e-01,\n",
      "          1.2541e-01,  3.3901e-01, -5.4468e-01,  4.8909e-03,  4.5487e-01,\n",
      "          3.0357e-01,  4.2399e-02, -9.3527e-01, -2.3600e-01, -1.0725e-01,\n",
      "         -2.8519e-01, -2.4515e-01,  4.6085e-01,  2.0427e-01,  2.9500e-01,\n",
      "          6.4376e-01, -3.3322e-01, -2.1801e-01, -2.0197e-01,  3.2985e-01,\n",
      "          2.8964e-01,  1.7433e-01, -1.3349e-01,  4.2327e-01,  6.0194e-02,\n",
      "          3.5433e-01, -3.8786e-01,  3.0927e-01, -3.0223e-01,  3.4954e-01,\n",
      "          1.3972e-01,  2.3326e-01, -2.9953e-01,  3.2260e-01,  2.8133e-01,\n",
      "          1.6747e-01, -4.2954e-01, -1.1423e-01,  2.2644e-01, -2.4675e-01,\n",
      "         -2.9961e-01,  4.0956e-01, -2.3440e-02, -5.7656e-01,  4.9329e-02,\n",
      "          1.8212e-01,  5.1535e-01, -3.4070e-02,  1.2431e-01, -3.0599e-01,\n",
      "          2.1696e-01,  1.1876e-02, -6.1342e-01, -2.2697e-01, -5.4285e-01,\n",
      "          4.0716e-01,  7.5690e-01,  5.2925e-01,  3.3127e-01, -8.4788e-01,\n",
      "          3.5865e-01, -5.1255e-01,  4.5703e-01,  8.6646e-02,  6.5914e-02,\n",
      "         -1.4184e-01, -4.3240e-02,  4.3998e-01,  1.9092e-01, -4.4292e-01,\n",
      "          3.7237e-01,  4.1200e-01, -4.3412e-01, -1.7135e-01,  2.6896e-01,\n",
      "         -2.1057e-01,  2.7867e-01,  8.4261e-02,  6.8961e-01, -1.0226e-01,\n",
      "         -2.5096e-01,  3.8952e-01,  3.4222e-01, -5.8961e-01,  5.4080e-01,\n",
      "          4.5628e-01,  5.9977e-02,  2.2767e-01, -3.3666e-01,  6.2863e-02,\n",
      "         -1.9291e-01, -4.2986e-01,  3.8287e-02, -3.3978e-01, -2.6447e-01,\n",
      "          4.0920e-01,  2.7006e-02, -3.0952e-01],\n",
      "        [-4.0734e-01,  1.4817e-01,  6.3463e-01,  2.8576e-03,  1.0364e-01,\n",
      "          6.1503e-01,  2.1092e-01,  1.4703e-01, -4.8067e-01, -5.6078e-02,\n",
      "          3.1822e-01, -4.6974e-01, -3.0376e-01,  2.3078e-01,  2.2220e-01,\n",
      "         -9.8986e-02,  3.0590e-02, -1.4811e-01,  2.9605e-01, -7.4225e-02,\n",
      "         -2.2037e-01, -2.0129e-01,  1.6279e-01, -5.0947e-01, -5.7031e-01,\n",
      "          6.5659e-02, -4.9973e-02, -3.1743e-01, -1.8447e-01,  2.7400e-01,\n",
      "         -8.1696e-02,  4.4057e-01,  1.6410e-02,  3.3255e-01,  2.4781e-01,\n",
      "          2.9010e-01, -5.8091e-02, -6.4710e-01, -2.2269e-01,  9.8599e-02,\n",
      "         -1.4653e-01, -3.6658e-02,  3.7091e-01,  2.1644e-01,  3.3239e-01,\n",
      "          2.7416e-01, -9.5445e-02,  9.3443e-02, -2.6674e-01,  1.9855e-01,\n",
      "          3.9673e-01,  4.8609e-01,  5.9685e-02,  2.8955e-01, -2.1033e-01,\n",
      "         -6.3462e-02, -4.6988e-01,  4.8361e-01,  2.1868e-02, -8.6698e-02,\n",
      "          3.8480e-01, -2.8208e-02, -4.7943e-01,  3.5649e-01, -7.9196e-03,\n",
      "          2.9618e-01, -3.5405e-01,  8.9395e-02,  1.8502e-01, -1.0049e-02,\n",
      "         -4.8404e-01,  3.4394e-01,  2.3919e-01, -3.5365e-01,  2.0400e-01,\n",
      "          3.5861e-01,  4.5107e-01,  1.9198e-01,  4.1042e-02, -6.8441e-01,\n",
      "         -3.3095e-01,  6.0498e-02, -2.3625e-01, -1.6576e-01, -6.0085e-01,\n",
      "          2.7843e-01,  3.7890e-01,  5.4439e-01,  7.0424e-02, -5.1366e-01,\n",
      "          7.6895e-01, -3.4365e-01,  6.6022e-01,  1.2227e-01, -3.6774e-01,\n",
      "         -1.2245e-01, -2.6067e-01,  4.3112e-02, -4.6068e-01, -6.5880e-01,\n",
      "          2.2435e-01,  7.4779e-02, -8.0925e-01,  2.0762e-01,  1.2752e-01,\n",
      "          3.4185e-02,  1.3048e-01, -3.2974e-01,  3.2554e-01, -5.5435e-01,\n",
      "         -3.6020e-01,  1.5720e-01, -6.3420e-02, -4.5800e-01,  4.4596e-01,\n",
      "          2.2049e-01,  1.9086e-01, -2.3870e-01,  4.5708e-03,  5.3129e-01,\n",
      "         -3.0728e-01, -4.6804e-01,  5.3120e-04, -9.6354e-02, -5.6920e-01,\n",
      "         -3.3553e-02,  1.8249e-01, -9.4930e-02],\n",
      "        [-6.1032e-01,  2.2592e-02,  7.8503e-01,  1.4663e-01, -7.3064e-02,\n",
      "          7.0944e-01,  3.8324e-01,  6.8072e-01, -3.7731e-01,  2.2451e-01,\n",
      "         -1.2764e-01, -5.3535e-01, -7.1868e-01, -7.4134e-02, -4.4237e-02,\n",
      "         -1.0581e-01,  1.7158e-01,  5.7950e-02,  1.9755e-01, -3.7925e-01,\n",
      "         -6.6799e-01, -1.6051e-01,  2.9078e-01, -6.6561e-01, -5.2135e-01,\n",
      "          2.7500e-01, -2.8117e-01, -4.7855e-01, -1.6271e-01,  1.8325e-01,\n",
      "          5.5243e-03,  7.1189e-01, -2.7342e-01, -3.1759e-01,  4.5552e-01,\n",
      "          7.3119e-02,  6.8161e-02, -1.1521e+00, -4.1795e-01,  2.5828e-01,\n",
      "         -4.0062e-01,  8.7341e-02,  4.2855e-01,  2.5169e-01,  1.8327e-01,\n",
      "          7.0269e-01, -4.2148e-01, -6.6923e-02, -1.3294e-01,  4.2429e-02,\n",
      "          7.2356e-01,  2.2604e-01, -4.6018e-01,  2.0352e-01, -4.5411e-02,\n",
      "          1.7137e-01, -6.2582e-01,  5.1745e-01, -2.7979e-01,  3.9837e-01,\n",
      "          9.9070e-02,  1.3258e-01, -1.6493e-01,  7.6729e-01, -2.8673e-03,\n",
      "          3.5647e-01, -7.5243e-01, -8.8054e-02,  5.6373e-01, -1.0608e-01,\n",
      "         -2.6105e-01,  4.9139e-01, -1.9327e-01, -7.0786e-01, -1.0717e-01,\n",
      "          6.5276e-02,  7.5842e-01,  7.5150e-02, -3.0350e-02, -6.8102e-01,\n",
      "          2.4900e-02,  2.3445e-01, -9.0493e-01, -5.9409e-02, -6.9971e-01,\n",
      "          7.1355e-01,  8.6844e-01,  6.3683e-01,  5.3230e-01, -1.2305e+00,\n",
      "          6.3627e-01, -5.8548e-01,  6.2375e-01, -7.2690e-02, -6.2468e-02,\n",
      "         -1.8366e-02, -2.7238e-01,  2.3060e-01,  7.4473e-02, -7.7266e-01,\n",
      "          2.2021e-01,  3.0954e-01, -7.5752e-01, -9.4634e-02,  4.4119e-01,\n",
      "         -3.4001e-01,  2.2902e-01,  1.3318e-01,  5.2592e-01, -2.3614e-01,\n",
      "         -4.0960e-01,  3.7773e-01,  9.8863e-02, -7.0757e-01,  2.9121e-01,\n",
      "          3.4120e-01, -6.8684e-02, -1.9249e-01, -5.2245e-01, -1.0039e-02,\n",
      "         -5.3607e-01, -5.8170e-01, -1.8071e-02, -2.9501e-01, -5.1746e-01,\n",
      "         -1.7658e-03,  3.1846e-01, -3.2815e-01]], grad_fn=<SubBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(-7.4506e-09, grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "class CenteredLayer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def forward(self, X):\n",
    "        return X - X.mean()\n",
    "layer = CenteredLayer()\n",
    "layer(torch.FloatTensor([1, 2, 3, 4, 5]))\n",
    "net = nn.Sequential(nn.Linear(8, 128), CenteredLayer())\n",
    "Y = net(torch.rand(4, 8))\n",
    "print(Y)\n",
    "Y.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "768d19a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0000],\n",
       "        [2.7123]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MyLinear(nn.Module):\n",
    "    def __init__(self, in_units, units):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_units, units))\n",
    "        self.bias = nn.Parameter(torch.randn(units,))\n",
    "    def forward(self, X):\n",
    "        linear = torch.matmul(X, self.weight.data) + self.bias.data\n",
    "        return F.relu(linear)\n",
    "linear = MyLinear(5, 3)\n",
    "linear.weight\n",
    "linear(torch.rand(2, 5))\n",
    "net = nn.Sequential(MyLinear(64, 8), MyLinear(8, 1))\n",
    "net(torch.rand(2, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23590a8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4, 2])\n",
      "tensor([[-1.5814, -4.6637],\n",
      "        [ 2.3336,  0.8622]], grad_fn=<CopySlices>)\n",
      "tensor([-6.2452,  3.1958], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "class Linear5_4_1(nn.Module):\n",
    "    def __init__(self, in_units, out_units):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.randn(in_units, in_units, out_units))\n",
    "    def forward(self, X):\n",
    "        b = X.shape[0] # b = batch_size\n",
    "        o = self.weight.shape[2] # o = out_units\n",
    "        y = torch.zeros(b,o) # y的shape为(batch_size，out_units)\n",
    "        for k in range(o):\n",
    "            for i in range(b):\n",
    "                # 矩阵乘法维度分别为：(1*4), (4*4), (4*1)\n",
    "                #print(torch.matmul(X[i,:],self.weight[:,:,k]))\n",
    "                #print(torch.matmul(torch.matmul(X[i,:],self.weight[:,:,k]),X[i,:]))\n",
    "                y[i,k] = torch.matmul(torch.matmul(X[i,:],self.weight[:,:,k]),X[i,:])\n",
    "                #print(y[i][k])\n",
    "                #print(y[i,k])\n",
    "        return y\n",
    "X = torch.randn(2, 4)\n",
    "linear = Linear5_4_1(4, 2)\n",
    "\n",
    "print(linear.weight.shape)\n",
    "print(linear(X))\n",
    "print(linear(X).sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8602d11d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "x = torch.arange(4)\n",
    "torch.save(x, 'x-file')\n",
    "x2 = torch.load('x-file')\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a0b3449",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.zeros(4)\n",
    "torch.save([x, y],'x-files')\n",
    "x2, y2 = torch.load('x-files')\n",
    "(x2, y2)\n",
    "mydict = {'x': x, 'y': y}\n",
    "torch.save(mydict, 'mydict')\n",
    "mydict2 = torch.load('mydict')\n",
    "mydict2['x']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "311b249a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.1496,  0.0502, -0.0438,  0.1878, -0.1688, -0.0956,  0.2382,  0.1080,\n",
      "         -0.0609,  0.1702],\n",
      "        [ 0.0398, -0.2328, -0.2326, -0.0437, -0.0265, -0.1886,  0.0265,  0.0865,\n",
      "         -0.0544,  0.1310]], grad_fn=<AddmmBackward0>)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.Linear(20, 256)\n",
    "        self.output = nn.Linear(256, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.output(F.relu(self.hidden(x)))\n",
    "\n",
    "net = MLP()\n",
    "X = torch.randn(size=(2, 20))\n",
    "Y = net(X)\n",
    "print(Y)\n",
    "torch.save(net.state_dict(), 'mlp.params')\n",
    "clone = MLP()\n",
    "clone.load_state_dict(torch.load('mlp.params'))\n",
    "clone.eval()\n",
    "Y_clone = clone(X)\n",
    "Y_clone == Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d0b311ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OrderedDict([('weight', tensor([[ 0.0564,  0.0267,  0.0545,  ..., -0.0380,  0.0343,  0.0190],\n",
      "        [-0.0127, -0.0444, -0.0539,  ..., -0.0445,  0.0140,  0.0315],\n",
      "        [ 0.0235,  0.0286,  0.0332,  ..., -0.0583, -0.0183, -0.0211],\n",
      "        ...,\n",
      "        [-0.0321, -0.0391,  0.0431,  ..., -0.0465, -0.0060,  0.0504],\n",
      "        [-0.0546,  0.0160, -0.0326,  ..., -0.0229,  0.0531, -0.0302],\n",
      "        [-0.0409,  0.0568, -0.0100,  ...,  0.0009, -0.0478,  0.0048]])), ('bias', tensor([-0.0223,  0.0433,  0.0002,  0.0180,  0.0437,  0.0105,  0.0472, -0.0081,\n",
      "         0.0019,  0.0508]))])\n"
     ]
    }
   ],
   "source": [
    "torch.save(net.output.state_dict(), 'mlp.out_put_params')\n",
    "params=torch.load('mlp.out_put_params')\n",
    "print(params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c5ac15fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        ...,\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False],\n",
      "        [False, False, False,  ..., False, False, False]])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        ...,\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True],\n",
       "        [True, True, True,  ..., True, True, True]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clone2=MLP()\n",
    "clone2.eval()\n",
    "clone2.output.load_state_dict(torch.load('mlp.out_put_params'))\n",
    "print(clone2.hidden.weight==net.hidden.weight)\n",
    "clone2.output.weight==net.output.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0e367b21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP(\n",
      "  (hidden): Linear(in_features=20, out_features=256, bias=True)\n",
      "  (output): Linear(in_features=256, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "#连模型一块保存\n",
    "model = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))\n",
    "torch.save(clone2, 'model.pt')\n",
    "m = torch.load('model.pt',weights_only=False)\n",
    "print(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b62a6dc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
